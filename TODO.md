# æ ¸å¿ƒåŸåˆ™

## è´¨é‡ç¬¬ä¸€
- å®å¯å¤šèŠ±æ—¶é—´ï¼Œä¹Ÿè¦ä¿è¯ä»£ç è´¨é‡
- å……åˆ†æ€è€ƒã€åˆ†æåå†åŠ¨æ‰‹å®ç°
- ä¸è¦ä¸ºäº†å¿«é€Ÿå®Œæˆè€Œç‰ºç‰²ä»£ç è´¨é‡

## åˆ†æ­¥å®Œæˆ
- å¦‚æœå½“å‰å¯¹è¯æ— æ³•å®Œæˆæ‰€æœ‰åŠŸèƒ½ï¼Œä¸»åŠ¨æ‹†åˆ†ä¸ºå¤šè½®å¯¹è¯
- æ¯è½®åªä¸“æ³¨å®Œæˆä¸€ä¸ªæ¸…æ™°çš„ç›®æ ‡
- ä¸è´ªå¤šï¼Œç¡®ä¿æ¯ä¸€æ­¥éƒ½é«˜è´¨é‡å®Œæˆ

## å……åˆ†è°ƒç ”
- å¦‚æœ‰éœ€è¦ï¼Œå……åˆ†ã€å½»åº•åœ°æœç´¢å’Œè°ƒç ”
- åˆ†æå’ŒæŒæ¡ç°æœ‰çš„é«˜è´¨é‡åŠŸèƒ½å®ç°å’Œç®—æ³•
- å€Ÿé‰´ä¸šç•Œæœ€ä½³å®è·µï¼Œä¸è¦é—­é—¨é€ è½¦

## è°ƒè¯•æ”¯æŒ
- å¦‚æœ‰éœ€è¦ï¼Œå¯ä»¥åŠ å…¥ debug/logging å‡½æ•°è¾…åŠ©å¼€å‘
- é€šè¿‡æ—¥å¿—è¾“å‡ºå¸®åŠ©å®šä½å’Œè§£å†³é—®é¢˜
- è°ƒè¯•ä»£ç å¯åœ¨åŠŸèƒ½ç¨³å®šåæ ‡æ³¨æˆ–ç§»é™¤

## ä»£ç è´¨é‡  
- æ³¨æ„ä»£ç å°½å¯èƒ½æ¨¡å—åŒ–è®¾è®¡ï¼ŒèŒè´£å°½å¯èƒ½çš„åˆ†ç¦»ï¼Œä¸è¦æŠŠæ‰€æœ‰ä»£ç å†™åœ¨ä¸€ä¸ªæ–‡ä»¶é‡Œï¼Œä¸æ–¹ä¾¿åç»­ç†è§£å’Œç»´æŠ¤  
- æ³¨æ„ä»£ç çš„å¤ç”¨æ€§ï¼Œä¸è¦å†™é‡å¤çš„ä»£ç   

## æ²Ÿé€šè§„èŒƒ
- **å¼€å§‹å‰**ï¼šè¯´æ˜ä½ ç†è§£çš„ä»»åŠ¡ç›®æ ‡å’Œå°†éµå®ˆçš„è§„åˆ™
- **è¿›è¡Œä¸­**ï¼šå¦‚éœ€æ‹†åˆ†ï¼Œæ˜ç¡®å‘ŠçŸ¥æœ¬è½®å°†å®Œæˆä»€ä¹ˆ
- **å®Œæˆå**ï¼šæ€»ç»“æœ¬è½®æˆæœï¼Œè¯´æ˜åç»­è®¡åˆ’ï¼ˆå¦‚æœ‰ï¼‰  


æµ‹è¯•ç¯å¢ƒä¸º**py310**  


# TODO  
~~åˆ¤åˆ«å™¨åŠ å…¥åæ•ˆæœæ— æå‡åˆ†æ~~ **å·²å®Œæˆåˆ†æå’Œä¿®å¤ (2026-02-28 v2)**

## åˆ†æç»“è®º

### ğŸ”´ Bug1ï¼ˆå·²ä¿®å¤ï¼‰ï¼šéªŒè¯å’Œå¯è§†åŒ–ä½¿ç”¨åœ¨çº¿æƒé‡è€Œé EMA æƒé‡
- EMA æƒé‡ç†è®ºä¸Šæ›´å¥½ï¼Œä½†ä»æœªç”¨äºéªŒè¯/å¯è§†åŒ–/best checkpoint é€‰æ‹©
- ç”¨æˆ·çœ‹åˆ°çš„"æ¨¡ç³Š"å¯èƒ½éƒ¨åˆ†å› ä¸ºæ²¡ç”¨ EMA

### ğŸ”´ é—®é¢˜2ï¼ˆéœ€å®éªŒç¡®è®¤ï¼‰ï¼šG æ€» loss ä¸­ FM è¿‡å¼ºï¼ŒGAN ä¿¡å·è¢«æ·¹æ²¡
- é‡å»º loss: 0.67 (47%)ï¼ŒGAN: 0.26 (18%)ï¼ŒFM: 0.50 (35%)
- GAN æ˜¯æ¨åŠ¨æ¸…æ™°åº¦çš„ä¿¡å·ï¼Œä½†ä»…å  18%
- FM æœ¬è´¨æ˜¯ D ç‰¹å¾ç©ºé—´çš„ L1 æ­£åˆ™ï¼Œä¸äº§ç”Ÿé”åŒ–æ•ˆæœ

### ğŸŸ¡ é—®é¢˜3ï¼šD æ¢¯åº¦èŒƒæ•°æé«˜ï¼ˆ40â†’27ï¼‰ï¼Œgrad_clip=5.0 è£æ‰ 80%+
- D ä¿¡å·æä¸ç¨³å®šï¼Œéœ€è¦é€šè¿‡ D_real/D_fake å‡å€¼æ¥ç¡®è®¤ D æ˜¯å¦æœ‰æ•ˆ

### ğŸŸ¡ é—®é¢˜4ï¼ˆå·²ä¿®å¤ï¼‰ï¼špretrained_G å LR warmup ä»é›¶é‡å¯
- G åœ¨å‰ ~2 epochs å‡ ä¹ä¸å­¦ä¹ ï¼Œå¯¼è‡´ val_loss æš‚æ—¶æ¶åŒ–
- æ–°å¢ `skip_warmup: true` é…ç½®é€‰é¡¹

## å·²å®æ–½ä¿®å¤
1. `trainer.py` â€” **EMA éªŒè¯**ï¼šéªŒè¯å’Œå¯è§†åŒ–æ”¹ç”¨ EMA æƒé‡ï¼ˆ_swap_ema_weights/_restore_model_weightsï¼‰
2. `trainer.py` â€” **D è¯Šæ–­æ—¥å¿—**ï¼šæ–°å¢ D_realã€D_fakeï¼ˆD å¯¹çœŸ/å‡å›¾çš„å¹³å‡è¾“å‡ºï¼‰ã€w_reconã€w_ganã€w_fmï¼ˆåŠ æƒ loss ç»„æˆï¼‰
3. `config.py` â€” **skip_warmup é€‰é¡¹**ï¼š`skip_warmup: true` è·³è¿‡ LR warmup

## ä¸‹ä¸€æ­¥å®éªŒï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

### å®éªŒ1ï¼šEMA + skip_warmup é‡è®­ G+Dï¼ˆæœ€å°æ”¹åŠ¨éªŒè¯ï¼‰
åœ¨ config.yaml ä¸­åŠ å…¥ `skip_warmup: true`ï¼Œå…¶ä»–ä¸å˜ï¼Œé‡æ–°è®­ç»ƒ G+Dã€‚
è§‚å¯Ÿæ—¥å¿—ä¸­çš„ `D_real` å’Œ `D_fake`ï¼š
- å¦‚æœ D_real â‰ˆ D_fakeï¼ˆD æ— æ³•åŒºåˆ†çœŸå‡ï¼‰â†’ D æœ¬èº«æ— æ•ˆ
- å¦‚æœ D_real >> D_fakeï¼ˆD æœ‰æ•ˆï¼‰ä½†å›¾åƒä»æ¨¡ç³Š â†’ loss æƒé‡æœ‰é—®é¢˜

### å®éªŒ2ï¼šé™ä½ FM æƒé‡ï¼ˆå¦‚æœå®éªŒ1æ˜¾ç¤º D æœ‰æ•ˆä½†ä»æ¨¡ç³Šï¼‰
å°† `feat_match_weight` ä» 10.0 é™åˆ° 2.0ï¼Œè®© GAN ä¿¡å·å æ¯”æå‡åˆ° 30%+ã€‚

### å®éªŒ3ï¼šæé«˜ GAN æƒé‡ï¼ˆå¦‚æœå®éªŒ2ä»ä¸å¤Ÿï¼‰
å°† `gan_weight` ä» 1.0 æåˆ° 2.0-5.0ã€‚æ³¨æ„å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚

### å®éªŒ4ï¼šG-only è®­ç»ƒæ›´ä¹…ï¼ˆå¯¹æ¯”åŸºçº¿ï¼‰
G-only åœ¨ E30 ä»åœ¨æ”¶æ•›ï¼Œå¯è®­ç»ƒåˆ° 100+ epochs çœ‹ loss æ˜¯å¦è¿˜èƒ½ä¸‹é™ã€‚  





**æœªæ¥å¯èƒ½è®¡åˆ’ï¼Œæš‚æ—¶ä¸ç”¨å®ç°**  
Perceptual Loss (VGG/LPIPS): Add a perceptual loss using pre-trained VGG features. This is especially effective for generating realistic textures in medical images. Since the data is single-channel grayscale, you'd replicate to 3 channels before feeding into VGG. Weight ~0.1-1.0 relative to L1.  

Progressive Training Strategy: Train in phases:
Phase 1: G only with L1+SSIM (50 epochs)
Phase 2: Enable R (50 epochs)
Phase 3: Enable D (100+ epochs)
This prevents GAN instability early on and lets G converge to a reasonable baseline first.  

Mixed Precision (AMP): Use torch.cuda.amp for the generator forward/backward pass. The UNet is large (61M params) and AMP would roughly halve memory and speed up training 1.5-2x without quality loss.  

Gradient Penalty (R1): Instead of or in addition to SN, consider R1 gradient penalty on the discriminator. This provides a more direct regularization and is used in StyleGAN-family models. Typical weight: 10.0.  

Attention in Registration Net: The current registration UNet is very lightweight (0.07M). For cases with complex misalignments, adding a single self-attention layer at the bottleneck could help capture long-range spatial correspondences.  

Multi-resolution Loss: Compute L1+SSIM at multiple resolutions (original + 2x/4x downsampled). This helps the generator learn both fine detail and global structure simultaneously.  

Curriculum on Lung Weight: Start with lung_weight=1.0 and gradually increase to 10.0 over the first 20-30 epochs. A sudden 10x emphasis on lung regions might cause early instability.  

Test-Time Augmentation (TTA): During inference, run the model with 2-4 augmented versions (flips, small rotations) of the input and average predictions. This typically improves SSIM/PSNR by 0.5-1.0 dB at the cost of proportional inference time.